{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e267f39",
   "metadata": {},
   "source": [
    "## Penjelasan Dataset yang digunakan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96544ff7",
   "metadata": {},
   "source": [
    "### 1. Kasus yang diambil:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880e73b",
   "metadata": {},
   "source": [
    "Kasus ini adalah Topic Modeling (Pemodelan Topik) dari judul berita bahasa Indonesia. Tujuannya adalah untuk menemukan topik-topik tersembunyi (kluster kata) yang ada di dalam ribuan judul berita secara otomatis (tanpa pengawasan/unsupervised).\n",
    "\n",
    "Kita akan menggunakan kolom tittle (judul) sebagai data teks utama. Kolom category dapat digunakan sebagai pembanding atau label untuk memvalidasi hasil visualisasi kita nanti (misalnya, apakah topik yang ditemukan sesuai dengan kategori yang ada)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdab0a",
   "metadata": {},
   "source": [
    "### 2. Penjelasan tiap kolom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146056b",
   "metadata": {},
   "source": [
    "Dataset indonesian-news-title.csv memiliki kolom:\n",
    "- date: Tanggal publikasi berita.\n",
    "- url: Tautan (URL) ke sumber berita.\n",
    "- tittle: Judul berita (ini akan menjadi teks utama yang kita analisis).\n",
    "- category: Kategori berita yang diberikan oleh sumber."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312883a",
   "metadata": {},
   "source": [
    "### 3. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fe5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re # Regular Expressions untuk membersihkan teks\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sastrawi untuk stemming dan stopword Bahasa Indonesia\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "# Sklearn untuk pemrosesan, modeling, dan visualisasi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Visualisasi\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Mengatur tampilan plot\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Download resource NLTK (hanya perlu sekali)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1377a12",
   "metadata": {},
   "source": [
    "### 4. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan path ke file Anda\n",
    "file_path = './dataset/indonesian-news-title.csv'\n",
    "\n",
    "# Muat dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Ganti nama 'tittle' menjadi 'title' agar lebih standar\n",
    "    if 'tittle' in df.columns:\n",
    "        df.rename(columns={'tittle': 'title'}, inplace=True)\n",
    "    \n",
    "    print(\"Data berhasil dimuat.\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' tidak ditemukan.\")\n",
    "    print(\"Pastikan file tersebut berada di direktori yang sama dengan notebook Anda, atau ubah 'file_path'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e93be",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abd61b",
   "metadata": {},
   "source": [
    "### 1. Jumlah baris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61303d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Jumlah total baris (judul berita): {len(df)}\")\n",
    "# Cara lain:\n",
    "print(f\"Bentuk data (baris, kolom): {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988655d8",
   "metadata": {},
   "source": [
    "### 2. Panjang rata-rata setiap baris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72460418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan kolom 'title' ada dan bertipe string\n",
    "if 'title' in df.columns:\n",
    "    df['title_length'] = df['title'].astype(str).apply(len)\n",
    "    print(f\"Panjang rata-rata judul berita: {df['title_length'].mean():.2f} karakter\")\n",
    "    \n",
    "    # Tampilkan 5 judul terpendek dan terpanjang\n",
    "    print(\"\\n5 Judul Terpendek:\")\n",
    "    print(df.sort_values(by='title_length').head()[['title', 'title_length']])\n",
    "    print(\"\\n5 Judul Terpanjang:\")\n",
    "    print(df.sort_values(by='title_length', ascending=False).head()[['title', 'title_length']])\n",
    "else:\n",
    "    print(\"Error: Kolom 'title' tidak ditemukan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdca59",
   "metadata": {},
   "source": [
    "### 3. Cek data duplikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'title' in df.columns:\n",
    "    duplicated_titles = df.duplicated(subset=['title']).sum()\n",
    "    print(f\"Jumlah judul berita yang duplikat: {duplicated_titles}\")\n",
    "    \n",
    "    # Opsional: Tampilkan beberapa data duplikat\n",
    "    if duplicated_titles > 0:\n",
    "        print(\"\\nContoh data duplikat:\")\n",
    "        print(df[df.duplicated(subset=['title'], keep=False)].sort_values(by='title').head())\n",
    "        \n",
    "    # Opsional: Hapus data duplikat jika diinginkan\n",
    "    # df_unique = df.drop_duplicates(subset=['title'])\n",
    "    # print(f\"Jumlah data setelah duplikat dihapus: {len(df_unique)}\")\n",
    "else:\n",
    "    print(\"Error: Kolom 'title' tidak ditemukan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63cede",
   "metadata": {},
   "source": [
    "### 4. Cek data kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95968d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cek data kosong (NaN/Null) per kolom:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e986e",
   "metadata": {},
   "source": [
    "### 5. Distribusi data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71342c1d",
   "metadata": {},
   "source": [
    "- Distribusi Kategori (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.countplot(y='category', data=df, order=df['category'].value_counts().index, palette='viridis')\n",
    "plt.title('Distribusi Kategori Berita', fontsize=16)\n",
    "plt.xlabel('Jumlah Berita', fontsize=12)\n",
    "plt.ylabel('Kategori', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e273e0a",
   "metadata": {},
   "source": [
    "- Kata yang Sering Muncul (Word Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'title' in df.columns:\n",
    "    # Gabungkan semua judul menjadi satu teks besar\n",
    "    all_titles = ' '.join(df['title'].astype(str).tolist())\n",
    "    \n",
    "    # Buat Word Cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_titles)\n",
    "    \n",
    "    # Tampilkan\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Kata yang Sering Muncul di Semua Judul Berita', fontsize=16)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Kolom 'title' tidak ditemukan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72850f37",
   "metadata": {},
   "source": [
    "## Data Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c547c",
   "metadata": {},
   "source": [
    "### 1. Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0606b3",
   "metadata": {},
   "source": [
    "### 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de904d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization (Menggunakan stemmer Sastrawi)\n",
    "def lemmatize_text(tokens):\n",
    "    # Di Sastrawi, stemming adalah proses mencari kata dasar (lemma)\n",
    "    print(\"Menggunakan Stemmer Sastrawi sebagai Lemmatizer...\")\n",
    "    # Kita butuh 'stemmer' dari langkah 3, pastikan langkah 3 sudah dijalankan\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31ef28",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Stemmer ---\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "# --- Definisi Fungsi Stemming ---\n",
    "# Stemming\n",
    "def stem_text(tokens):\n",
    "    # Stemming bekerja per kata (token)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098ce3b",
   "metadata": {},
   "source": [
    "### 4. Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49429274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Stopword ---\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "# Tambahkan stopwords kustom jika perlu\n",
    "more_stopwords = ['yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'k', 'ke', 'di', 'dari', 'ter', 'juta', 'rp']\n",
    "data_stopwords = stopword_factory.get_stop_words() + more_stopwords\n",
    "# Buat dictionary untuk Sastrawi\n",
    "dictionary = ArrayDictionary(data_stopwords)\n",
    "stopword_remover = StopWordRemover(dictionary)\n",
    "\n",
    "# --- Definisi Fungsi Stopword Removal ---\n",
    "# Stopword removal\n",
    "def remove_stopwords_internal(tokens):\n",
    "    # Sastrawi stopword remover bekerja pada string, bukan token\n",
    "    # Jadi kita gabung dulu lalu proses\n",
    "    text = ' '.join(tokens)\n",
    "    text_no_stop = stopword_remover.remove(text)\n",
    "    # Tokenisasi ulang setelah stopwords dihapus\n",
    "    return word_tokenize(text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260127b3",
   "metadata": {},
   "source": [
    "### 5. Text Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Normalisasi (termasuk case folding, hapus angka & tanda baca)\n",
    "def normalize_text(text):\n",
    "    text = text.lower() # Case folding\n",
    "    text = re.sub(r'\\d+', '', text) # Hapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Hapus tanda baca\n",
    "    text = text.strip() # Hapus spasi berlebih\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d04ad3",
   "metadata": {},
   "source": [
    "- Run text processing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe654d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Terapkan Pipeline ---\n",
    "print(\"Memulai Text Processing...\")\n",
    "if 'title' in df.columns:\n",
    "    # Ambil sampel data jika dataset terlalu besar (misal 5000 baris)\n",
    "    # df_sample = df.sample(5000, random_state=42).copy()\n",
    "    # Jika ingin proses semua data, gunakan baris di bawah:\n",
    "    df_sample = df.sample(500, random_state=42).copy()\n",
    "    \n",
    "    print(f\"Memproses {len(df_sample)} baris data... (Ini mungkin lambat)\")\n",
    "\n",
    "    # Terapkan pipeline\n",
    "    # (Biasanya kita pilih Stemming ATAU Lemmatization, di sini kita gunakan Stemming)\n",
    "    \n",
    "    # Langkah 5: Normalisasi\n",
    "    df_sample['clean_title'] = df_sample['title'].astype(str).apply(normalize_text)\n",
    "    # Langkah 1: Tokenisasi\n",
    "    df_sample['tokens'] = df_sample['clean_title'].apply(tokenize_text)\n",
    "    # Langkah 4: Stopword Removal\n",
    "    df_sample['tokens_no_stop'] = df_sample['tokens'].apply(remove_stopwords_internal)\n",
    "    \n",
    "    # Langkah 3: Stemming\n",
    "    # PERINGATAN: Stemming sangat lambat!\n",
    "    print(\"Memulai proses Stemming (Paling lama)...\")\n",
    "    df_sample['tokens_stemmed'] = df_sample['tokens_no_stop'].apply(stem_text)\n",
    "    \n",
    "    # Gabungkan kembali token yang sudah bersih menjadi kalimat\n",
    "    df_sample['final_text'] = df_sample['tokens_stemmed'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    print(\"Text Processing Selesai.\")\n",
    "    print(df_sample[['title', 'final_text']].head())\n",
    "else:\n",
    "    print(\"Error: Kolom 'title' tidak ditemukan untuk diproses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4582397",
   "metadata": {},
   "source": [
    "### 6. Text Vektorisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d98e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi TF-IDF Vectorizer\n",
    "# max_df = 0.95 -> abaikan kata yang muncul di > 95% dokumen\n",
    "# min_df = 2 -> abaikan kata yang muncul di < 2 dokumen\n",
    "# max_features = 1000 -> ambil 1000 kata terpenting\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "\n",
    "# Terapkan vectorizer ke teks bersih kita\n",
    "tfidf_matrix = vectorizer.fit_transform(df_sample['final_text'])\n",
    "\n",
    "# Dapatkan nama-nama fitur (kata-kata)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Bentuk matriks TF-IDF: {tfidf_matrix.shape}\")\n",
    "print(f\"Contoh 10 fitur (kata): {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8543c",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ddb97",
   "metadata": {},
   "source": [
    "#### Penjelasan tentang model yang dipilih\n",
    "\n",
    "Kita akan menggunakan Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "LDA adalah model generatif probabilistik yang digunakan untuk topic modeling. Asumsi dasarnya adalah:\n",
    "\n",
    "1. Setiap dokumen (judul berita) adalah campuran dari berbagai topik.\n",
    "2. Setiap topik adalah campuran dari berbagai kata.\n",
    "\n",
    "LDA akan melihat matriks TF-IDF (atau CountVectorizer) dan mencoba mencari tahu:\n",
    "\n",
    "- Topik apa saja yang ada di seluruh dataset? (Misal: Topik A: 'polisi', 'tangkap', 'korupsi'; Topik B: 'saham', 'ihsg', 'rupiah')\n",
    "- Seberapa besar proporsi setiap topik dalam setiap dokumen? (Misal: Judul \"Polisi Tangkap Koruptor\" adalah 90% Topik A dan 10% Topik B).\n",
    "\n",
    "Kita perlu menentukan jumlah topik (n_components) yang ingin kita cari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan jumlah topik yang ingin dicari (misalnya 5 atau 10)\n",
    "# Ini adalah hyperparameter yang bisa disesuaikan\n",
    "NUM_TOPICS = 10 \n",
    "\n",
    "# Buat model LDA\n",
    "# Catatan: LDA secara teknis lebih baik bekerja dengan CountVectorizer,\n",
    "# tetapi TF-IDF juga bisa digunakan.\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=NUM_TOPICS, \n",
    "    max_iter=10, \n",
    "    learning_method='online', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Latih model LDA pada matriks TF-IDF\n",
    "print(\"Melatih model LDA...\")\n",
    "lda.fit(tfidf_matrix)\n",
    "print(\"Pelatihan LDA selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3829b27",
   "metadata": {},
   "source": [
    "## Visualisasi Topic yang dimunculkan menggunakan tabel, barchart dan sebagainya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fde931",
   "metadata": {},
   "source": [
    "### 1. Tabel Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "        topic_dict[f\"Topik {topic_idx+1}\"] = top_words\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "num_top_words = 10\n",
    "topics_df = display_topics(lda, feature_names, num_top_words)\n",
    "\n",
    "print(f\"Kata-kata Teratas untuk Setiap Topik (Top {num_top_words} Words):\")\n",
    "display(topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52fa93",
   "metadata": {},
   "source": [
    "### 2. Bar Chart untuk Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, topic_idx, n_top_words):\n",
    "    topic = model.components_[topic_idx]\n",
    "    top_words_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_indices]\n",
    "    top_words_values = [topic[i] for i in top_words_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_words_values, y=top_words, palette='coolwarm')\n",
    "    plt.title(f'Kata-kata Teratas untuk Topik #{topic_idx+1}', fontsize=16)\n",
    "    plt.xlabel('Bobot Kata (Weight)', fontsize=12)\n",
    "    plt.ylabel('Kata', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Visualisasikan 3 topik pertama\n",
    "plot_top_words(lda, feature_names, 0, 10) # Topik 1\n",
    "plot_top_words(lda, feature_names, 1, 10) # Topik 2\n",
    "plot_top_words(lda, feature_names, 2, 10) # Topik 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c0dec",
   "metadata": {},
   "source": [
    "### 3. Word Cloud untuk Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membuat dictionary frekuensi/bobot kata untuk satu topik\n",
    "def get_topic_word_weights(topic_component, feature_names):\n",
    "    # Membuat dictionary: {kata: bobot}\n",
    "    return {feature_names[i]: weight for i, weight in enumerate(topic_component)}\n",
    "\n",
    "# Tentukan berapa banyak topik yang ingin divisualisasikan\n",
    "# Ambil 6 topik pertama, atau semua jika total topik < 6\n",
    "num_topics_to_plot = min(NUM_TOPICS, 6) \n",
    "\n",
    "# Tentukan layout grid (misal: 2 baris, 3 kolom)\n",
    "ncols = 3\n",
    "nrows = (num_topics_to_plot + ncols - 1) // ncols # Menghitung jumlah baris yg dibutuhkan\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 4), squeeze=False)\n",
    "axes = axes.flatten() # Ratakan array axes agar mudah di-loop\n",
    "\n",
    "print(f\"\\nMenampilkan Word Cloud untuk {num_topics_to_plot} Topik Teratas...\")\n",
    "\n",
    "for i in range(num_topics_to_plot):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Dapatkan bobot kata untuk topik ke-i\n",
    "    topic_weights = get_topic_word_weights(lda.components_[i], feature_names)\n",
    "    \n",
    "    # Buat WordCloud dari frekuensi (bobot)\n",
    "    wc = WordCloud(\n",
    "        width=400, \n",
    "        height=300, \n",
    "        background_color='white', \n",
    "        colormap='viridis',\n",
    "        max_words=50 # Batasi jumlah kata agar tidak terlalu padat\n",
    "    ).generate_from_frequencies(topic_weights)\n",
    "    \n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(f'Topik {i+1}', fontsize=16)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Sembunyikan subplot yang tidak terpakai (jika jumlah topik ganjil)\n",
    "for i in range(num_topics_to_plot, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8194d5",
   "metadata": {},
   "source": [
    "## Gunakan implementasi menggunakan PCA atau T-sne untuk tahap lanjutan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e749af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memulai proses t-SNE... (Ini mungkin lambat)\")\n",
    "\n",
    "# --- Opsi 1: Langsung t-SNE (Mungkin lambat) ---\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
    "# tsne_results = tsne.fit_transform(tfidf_matrix.toarray()) # .toarray() butuh banyak memori\n",
    "\n",
    "# --- Opsi 2: TruncatedSVD (PCA) dulu, baru t-SNE (Lebih cepat dan stabil) ---\n",
    "# 1. Reduksi dimensi ke 50 dengan SVD (Sparse PCA)\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# 2. Terapkan t-SNE pada data 50 dimensi\n",
    "try:\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
    "except TypeError:\n",
    "    print(\"Peringatan: 'n_iter' tidak didukung oleh versi scikit-learn saat ini. Menggunakan nilai default iterasi.\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_results = tsne.fit_transform(svd_matrix)\n",
    "tsne_results = tsne.fit_transform(svd_matrix)\n",
    "\n",
    "print(\"Proses t-SNE selesai.\")\n",
    "\n",
    "# Buat DataFrame dari hasil t-SNE\n",
    "df_tsne = pd.DataFrame(tsne_results, columns=['tsne1', 'tsne2'])\n",
    "# Tambahkan kolom kategori dari data asli\n",
    "df_tsne['category'] = df_sample['category'].values\n",
    "\n",
    "# Visualisasikan hasil t-SNE\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne1\", y=\"tsne2\",\n",
    "    hue=\"category\",\n",
    "    palette=sns.color_palette(\"hsv\", len(df_tsne['category'].unique())),\n",
    "    data=df_tsne,\n",
    "    legend=\"full\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('Visualisasi t-SNE dari Judul Berita (diwarnai berdasarkan Kategori)', fontsize=16)\n",
    "plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
